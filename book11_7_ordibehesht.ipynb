{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOal8eK8rhrmD7zDhXeD4R4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaziehGolzari/special/blob/main/book11_7_ordibehesht.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.7 Tagging Parts of Speech"
      ],
      "metadata": {
        "id": "ySaGfTxXtQKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTdB6VU3tLx0",
        "outputId": "c3648dc1-3fcb-4d05-d2c3-5e9cbbacaeb8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "# Create text\n",
        "text_data = \"Chris loved outdoor running\"\n",
        "# Use pretrained part of speech tagger\n",
        "text_tagged = pos_tag(word_tokenize(text_data))\n",
        "# Show parts of speech\n",
        "text_tagged"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCsJFh6HtTP_",
        "outputId": "75aff1c4-fc6e-4093-aecd-99a3926aaf2c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09lGOdGQs_-G",
        "outputId": "b2a4fdb4-f83e-44dd-acd9-7e016acbe099"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chris']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Filter words\n",
        "[word for word, tag in text_tagged if tag in ['NN','NNS','NNP','NNPS'] ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "# Create text\n",
        "tweets = [\"I am eating a burrito for breakfast\",\n",
        "\"Political science is an amazing field\",\n",
        "\"San Francisco is an awesome city\"]\n",
        "# Create list\n",
        "tagged_tweets = []\n",
        "# Tag each word and each tweet\n",
        "for tweet in tweets:\n",
        "    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
        "    tagged_tweets.append([tag for word, tag in tweet_tag])\n",
        "# Use one-hot encoding to convert the tags into features\n",
        "one_hot_multi = MultiLabelBinarizer()\n",
        "one_hot_multi.fit_transform(tagged_tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzkRfdgttlam",
        "outputId": "8065cfff-1030-4547-801a-7bbc4e3e9b46"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
              "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show feature names\n",
        "one_hot_multi.classes_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAi4-N-Etqkk",
        "outputId": "2d8c60ec-ecd8-4066-f6c8-b51917efd02f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.8 Performing Named-Entity Recognition"
      ],
      "metadata": {
        "id": "N15JeR3utyhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load the spaCy package and use it to parse the text\n",
        "# make sure you have run \"python -m spacy download en\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Elon Musk offered to buy Twitter using $21B of his own money.\")\n",
        "# Print each entity\n",
        "print(doc.ents)\n",
        "# For each entity print the text and the entity label\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_, sep=\",\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHo5HxZFtuxH",
        "outputId": "4048466c-9ace-4feb-b296-2666077d9aa5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Elon Musk, Twitter, 21B)\n",
            "Elon Musk,PERSON\n",
            "Twitter,PERSON\n",
            "21B,MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.9 Encoding Text as a Bag of Words"
      ],
      "metadata": {
        "id": "1HoyVEXmt-sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load library\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Create text\n",
        "text_data = np.array(['I love Brazil. Brazil!',\n",
        "'Sweden is best',\n",
        "'Germany beats both'])\n",
        "# Create the bag of words feature matrix\n",
        "count = CountVectorizer()\n",
        "bag_of_words = count.fit_transform(text_data)\n",
        "# Show feature matrix\n",
        "bag_of_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_y_3yDHt4l_",
        "outputId": "26e3a9f0-905e-43f7-eec7-d6cc08ab4cba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
              "\twith 8 stored elements and shape (3, 8)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pIc3yk-uDt7",
        "outputId": "e822f1ad-a82f-463d-d867-6b025315bb7a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
              "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
              "       [1, 0, 1, 0, 1, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show feature names\n",
        "count.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXzjQNOUuH-V",
        "outputId": "09a8e884-b944-4cd8-9d61-667ef0e004ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love',\n",
              "       'sweden'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature matrix with arguments\n",
        "count_2gram = CountVectorizer(ngram_range=(1,2),\n",
        "stop_words=\"english\",\n",
        "vocabulary=['brazil'])\n",
        "bag = count_2gram.fit_transform(text_data)\n",
        "# View feature matrix\n",
        "bag.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSN9wek3uRUZ",
        "outputId": "f7e5b60b-620e-40bf-d1ab-2612604d84c4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2],\n",
              "       [0],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View the 1-grams and 2-grams\n",
        "count_2gram.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwkjyENyuWZJ",
        "outputId": "506b4a19-ba8a-448a-dae1-7cde3b4fb725"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'brazil': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.10 Weighting Word Importance\n"
      ],
      "metadata": {
        "id": "srf71Rz1uMW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Create text\n",
        "text_data = np.array(['I love Brazil. Brazil!',\n",
        "'Sweden is best',\n",
        "'Germany beats both'])\n",
        "# Create the tf-idf feature matrix\n",
        "tfidf = TfidfVectorizer()\n",
        "feature_matrix = tfidf.fit_transform(text_data)\n",
        "# Show tf-idf feature matrix\n",
        "feature_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vdl7zl9uoFM",
        "outputId": "31c08053-5090-444a-af69-31184d17d7ed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
              "\twith 8 stored elements and shape (3, 8)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show tf-idf feature matrix as dense matrix\n",
        "feature_matrix.toarray()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t68kXqXQvI8m",
        "outputId": "f76ce18e-e9a2-472c-a55f-63cce82f89e0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
              "        0.        , 0.4472136 , 0.        ],\n",
              "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
              "        0.57735027, 0.        , 0.57735027],\n",
              "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027,\n",
              "        0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show feature names\n",
        "tfidf.vocabulary_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX48kxeJvPNV",
        "outputId": "ea60811a-0f25-4f97-d364-b607340d4890"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'love': 6,\n",
              " 'brazil': 3,\n",
              " 'sweden': 7,\n",
              " 'is': 5,\n",
              " 'best': 1,\n",
              " 'germany': 4,\n",
              " 'beats': 0,\n",
              " 'both': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6rC7X0mPvU34"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}